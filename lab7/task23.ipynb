{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63fe2d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix, save_npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "666376e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1449936, Val: 181242, Test: 181243\n"
     ]
    }
   ],
   "source": [
    "with open(\"../lab4/telugu_tokenized_sentences.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    sentences = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "train_sentences, temp_sentences = train_test_split(sentences, test_size=0.2, random_state=42)\n",
    "val_sentences, test_sentences = train_test_split(temp_sentences, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train_sentences)}, Val: {len(val_sentences)}, Test: {len(test_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80077ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vocabulary size: 728479\n"
     ]
    }
   ],
   "source": [
    "def tokenize(s): return s.split()\n",
    "\n",
    "vocab = set()\n",
    "for s in train_sentences:\n",
    "    vocab.update(tokenize(s))\n",
    "vocab = sorted(vocab)\n",
    "vocab_index = {w: i for i, w in enumerate(vocab)}\n",
    "V = len(vocab)\n",
    "print(f\"âœ… Vocabulary size: {V}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d3e1f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… IDF computed\n"
     ]
    }
   ],
   "source": [
    "def compute_idf(sentences, vocab_index):\n",
    "    N = len(sentences)\n",
    "    df = np.zeros(len(vocab_index))\n",
    "    for s in sentences:\n",
    "        for tok in set(tokenize(s)):\n",
    "            if tok in vocab_index:\n",
    "                df[vocab_index[tok]] += 1\n",
    "    return np.log((N + 1) / (df + 1)) + 1\n",
    "\n",
    "idf = compute_idf(train_sentences, vocab_index)\n",
    "print(\"âœ… IDF computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec37dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50000 sentences...\n",
      "Processed 100000 sentences...\n",
      "Processed 150000 sentences...\n",
      "Processed 200000 sentences...\n",
      "Processed 250000 sentences...\n",
      "Processed 300000 sentences...\n",
      "Processed 350000 sentences...\n",
      "Processed 400000 sentences...\n",
      "Processed 450000 sentences...\n",
      "Processed 500000 sentences...\n",
      "Processed 550000 sentences...\n",
      "Processed 600000 sentences...\n",
      "Processed 650000 sentences...\n",
      "Processed 700000 sentences...\n",
      "Processed 750000 sentences...\n",
      "Processed 800000 sentences...\n",
      "Processed 850000 sentences...\n",
      "Processed 900000 sentences...\n",
      "Processed 950000 sentences...\n",
      "Processed 1000000 sentences...\n",
      "Processed 1050000 sentences...\n",
      "Processed 1100000 sentences...\n",
      "Processed 1150000 sentences...\n",
      "Processed 1200000 sentences...\n",
      "Processed 1250000 sentences...\n",
      "Processed 1300000 sentences...\n",
      "Processed 1350000 sentences...\n",
      "Processed 1400000 sentences...\n",
      "Processed 50000 sentences...\n",
      "Processed 100000 sentences...\n",
      "Processed 150000 sentences...\n",
      "Processed 50000 sentences...\n",
      "Processed 100000 sentences...\n",
      "Processed 150000 sentences...\n",
      "âœ… Sparse TFâ€“IDF built!\n",
      "Train matrix: (1449936, 728479), nnz=16307238\n",
      "Val matrix:   (181242, 728479), nnz=1983153\n",
      "Test matrix:  (181243, 728479), nnz=1983941\n"
     ]
    }
   ],
   "source": [
    "def build_sparse_tfidf(sentences, vocab_index, idf):\n",
    "    row_idx, col_idx, data = [], [], []\n",
    "    for i, s in enumerate(sentences):\n",
    "        counts = Counter(tokenize(s))\n",
    "        total = len(tokenize(s))\n",
    "        for tok, cnt in counts.items():\n",
    "            if tok in vocab_index:\n",
    "                j = vocab_index[tok]\n",
    "                tf = cnt / total\n",
    "                val = tf * idf[j]\n",
    "                row_idx.append(i)\n",
    "                col_idx.append(j)\n",
    "                data.append(val)\n",
    "        if (i+1) % 50000 == 0:\n",
    "            print(f\"Processed {i+1} sentences...\")\n",
    "    return csr_matrix((data, (row_idx, col_idx)), shape=(len(sentences), len(vocab_index)))\n",
    "\n",
    "X_train = build_sparse_tfidf(train_sentences, vocab_index, idf)\n",
    "X_val   = build_sparse_tfidf(val_sentences, vocab_index, idf)\n",
    "X_test  = build_sparse_tfidf(test_sentences, vocab_index, idf)\n",
    "\n",
    "print(\"âœ… Sparse TF-IDF built!\")\n",
    "print(f\"Train matrix: {X_train.shape}, nnz={X_train.nnz}\")\n",
    "print(f\"Val matrix:   {X_val.shape}, nnz={X_val.nnz}\")\n",
    "print(f\"Test matrix:  {X_test.shape}, nnz={X_test.nnz}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f82b54c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Nearest Neighbor for each sentence (within same set)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea0c11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_neighbors(X):\n",
    "    n = X.shape[0]\n",
    "    nearest_indices = np.zeros(n, dtype=int)\n",
    "    nearest_scores = np.zeros(n)\n",
    "\n",
    "    batch_size = 5000\n",
    "    for start in range(0, n, batch_size):\n",
    "        end = min(start + batch_size, n)\n",
    "        sim = cosine_similarity(X[start:end], X)  # (batch_size, n)\n",
    "        np.fill_diagonal(sim[:, start:end], -1)   # avoid self-match in batch\n",
    "        idx = np.argmax(sim, axis=1)\n",
    "        score = np.max(sim, axis=1)\n",
    "        nearest_indices[start:end] = idx\n",
    "        nearest_scores[start:end] = score\n",
    "        print(f\"Processed {end}/{n} sentences\")\n",
    "\n",
    "    return nearest_indices, nearest_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e1f4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Finding nearest neighbors for Validation set...\n",
      "Processed 5000/181242 sentences\n"
     ]
    }
   ],
   "source": [
    "# Validation Set\n",
    "print(\"\\nðŸ”¹ Finding nearest neighbors for Validation set...\")\n",
    "val_nn_idx, val_nn_score = find_nearest_neighbors(X_val)\n",
    "print(\"âœ… Validation nearest neighbors found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc430496",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
